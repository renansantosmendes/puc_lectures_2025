{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Como Carregar um Modelo com Fine-Tuning LoRA**\n",
        "\n",
        "## **Visão Geral**\n",
        "\n",
        "Quando você realiza fine-tuning de um modelo usando LoRA (Low-Rank Adaptation), o resultado não é um modelo completo, mas sim um **adapter** — um conjunto pequeno de pesos que modificam o comportamento do modelo base original.\n",
        "\n",
        "Para usar o modelo ajustado, você precisa primeiro carregar o modelo base original e depois aplicar o adapter LoRA sobre ele.\n",
        "\n",
        "---\n",
        "\n",
        "## **Pré-requisitos**\n",
        "\n",
        "Antes de começar, certifique-se de ter instaladas as seguintes bibliotecas:\n",
        "\n",
        "- **transformers**: biblioteca principal do Hugging Face para trabalhar com modelos de linguagem\n",
        "- **peft**: biblioteca para carregar e aplicar adapters LoRA\n",
        "- **accelerate**: biblioteca para otimizar o carregamento e distribuição do modelo\n",
        "- **torch**: framework de deep learning do PyTorch\n",
        "\n",
        "---\n",
        "\n",
        "## **Passo a Passo**\n",
        "\n",
        "### **1. Identificar o Modelo Base Correto**\n",
        "\n",
        "O ponto mais importante é usar **exatamente a mesma versão** do modelo que foi utilizada durante o fine-tuning. Se você treinou com a versão \"Instruct\" de um modelo, deve carregar essa mesma versão. Usar uma versão diferente pode causar erros ou comportamentos inesperados.\n",
        "\n",
        "Por exemplo, se o fine-tuning foi feito com `Llama-3.2-1B-Instruct`, você deve carregar este modelo específico, e não a versão base `Llama-3.2-1B`.\n",
        "\n",
        "### **2. Carregar o Modelo Base**\n",
        "\n",
        "Utilize a classe `AutoModelForCausalLM` da biblioteca transformers para carregar o modelo base. É recomendado usar o parâmetro `device_map=\"auto\"` para que o modelo seja automaticamente distribuído entre GPU e CPU conforme a memória disponível. Também é aconselhável usar `torch_dtype=torch.float16` para reduzir o consumo de memória.\n",
        "\n",
        "O tokenizer deve ser carregado usando `AutoTokenizer` com o mesmo nome do modelo base.\n",
        "\n",
        "### **3. Aplicar o Adapter LoRA**\n",
        "\n",
        "Após carregar o modelo base, utilize a classe `PeftModel` da biblioteca peft para aplicar o adapter LoRA. Você precisa passar o modelo base já carregado e o caminho do adapter (que pode ser um repositório no Hugging Face Hub ou um caminho local).\n",
        "\n",
        "### **4. Mesclar os Pesos (Opcional)**\n",
        "\n",
        "Para obter inferência mais rápida, você pode mesclar os pesos do adapter diretamente no modelo base usando o método `merge_and_unload()`. Isso elimina o overhead do adapter durante a geração de texto, resultando em melhor performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **Realizando Inferência**\n",
        "\n",
        "Para gerar texto com o modelo carregado, você deve:\n",
        "\n",
        "1. Preparar as mensagens no formato de chat, incluindo as roles \"system\" e \"user\"\n",
        "2. Usar o método `apply_chat_template` do tokenizer para formatar as mensagens corretamente\n",
        "3. Enviar os tokens para o dispositivo correto (GPU)\n",
        "4. Chamar o método `generate` do modelo com os parâmetros desejados\n",
        "5. Decodificar a saída usando o tokenizer\n"
      ],
      "metadata": {
        "id": "pjDdSz1aEXBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U bitsandbytes unsloth"
      ],
      "metadata": {
        "id": "FBPu7hiIFwyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-3.2-3B-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"renansantosmendes/synapseai-llama3.2-3B-instruct-lora-v3\"\n",
        ")"
      ],
      "metadata": {
        "id": "Q1h3-WiaFeNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "-2ElkTgl9rqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "38fC4ILl_tUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Chat interface module for SynapseAI Solutions language model.\"\"\"\n",
        "\n",
        "from typing import Any\n",
        "import torch\n",
        "from pydantic import BaseModel, Field\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "from typing import Any\n",
        "import torch\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class SynapseAIChatbot(BaseModel):\n",
        "    \"\"\"A chatbot interface for SynapseAI Solutions language model.\"\"\"\n",
        "\n",
        "    model_config = {\"arbitrary_types_allowed\": True}\n",
        "\n",
        "    model: Any\n",
        "    tokenizer: Any\n",
        "    max_new_tokens: int = Field(default=256, ge=1, le=4096)\n",
        "    device: str = \"cuda\"\n",
        "    system_prompt: str = (\n",
        "        \"Você é um assistente corporativo da SynapseAI Solutions, \"\n",
        "        \"uma empresa especializada em produtos de IA Generativa para Finanças, \"\n",
        "        \"Educação e Saúde. Você responde de forma técnica, precisa e profissional, \"\n",
        "        \"sempre alinhado às capacidades dos produtos FinBrain, RiskGen, \"\n",
        "        \"EduMentor AI, CourseGen Studio e ClinicaGPT. Você respeita LGPD, \"\n",
        "        \"compliance regulatório e uso responsável de IA.\"\n",
        "    )\n",
        "    history: list[dict[str, str]] = Field(default_factory=list)\n",
        "\n",
        "    def model_post_init(self, __context) -> None:\n",
        "        \"\"\"Initialize conversation with system prompt after model creation.\"\"\"\n",
        "        if not self.history:\n",
        "            self.history.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"Send a message and receive a response from the chatbot.\n",
        "\n",
        "        Args:\n",
        "            user_message: The user's input message.\n",
        "\n",
        "        Returns:\n",
        "            Only the assistant's generated response.\n",
        "        \"\"\"\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            conversation=self.history,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = torch.ones_like(inputs)\n",
        "        input_length = inputs.shape[1]\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            max_new_tokens=self.max_new_tokens\n",
        "        )\n",
        "\n",
        "        # Extrair apenas os tokens gerados\n",
        "        new_tokens = outputs[0][input_length:]\n",
        "        assistant_response = self.tokenizer.decode(\n",
        "            new_tokens,\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        return assistant_response\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Clear conversation history and reinitialize with system prompt.\"\"\"\n",
        "        self.history.clear()\n",
        "        self.history.append({\"role\": \"system\", \"content\": self.system_prompt})"
      ],
      "metadata": {
        "id": "heiAPonRuA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "chatbot = SynapseAIChatbot(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "response = chatbot.chat(\"O que é o FinBrain?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Kf0DvRnZuF52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.reset()\n",
        "response = chatbot.chat(\"O que é a synapse ai?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "45CJCo_UIfSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.reset()\n",
        "response = chatbot.chat(\"Quais métricas de observabilidade o RiskGen possui?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "qDk9Trlf07ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.reset()\n",
        "response = chatbot.chat(\"A documentação técnica do RiskGen está onde?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "NY_W-H556wFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLQzy0Sn7bjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}