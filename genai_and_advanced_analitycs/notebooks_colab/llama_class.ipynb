{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e684e05e9884e05a684a7a6a2d479c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485314ff43c84f1795ec62f639e9f4b4",
              "IPY_MODEL_efef74cc19e64d24b50b25980457401e",
              "IPY_MODEL_c71e7b435a7a437d99e62b9c060bccb9"
            ],
            "layout": "IPY_MODEL_32a3d65ade6541408f3c937301ec08a2"
          }
        },
        "485314ff43c84f1795ec62f639e9f4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_157dad2c04884223b9ceeee553625de8",
            "placeholder": "​",
            "style": "IPY_MODEL_0d8f4ec9e6e54e50af980ab4ab26af86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "efef74cc19e64d24b50b25980457401e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abba06bccc2e490abe0cb68119a62f89",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e4fc03b5764235aea3383f4274e9f1",
            "value": 4
          }
        },
        "c71e7b435a7a437d99e62b9c060bccb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8825e10ddc974599850f213aace73229",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8dbb4f6fb44483a8f3e51f48258a8f",
            "value": " 4/4 [01:08&lt;00:00, 15.17s/it]"
          }
        },
        "32a3d65ade6541408f3c937301ec08a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157dad2c04884223b9ceeee553625de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8f4ec9e6e54e50af980ab4ab26af86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abba06bccc2e490abe0cb68119a62f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e4fc03b5764235aea3383f4274e9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8825e10ddc974599850f213aace73229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8dbb4f6fb44483a8f3e51f48258a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IIz16-D5eks",
        "outputId": "094e9eae-69a7-4c21-d7ba-5c1b25cd25a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g929fUPV5X6I"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "# from torch.quantization import quantize_dynamic\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# access_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_8bit=True,\n",
        "#     llm_int8_threshold=6.0,\n",
        "#     llm_int8_skip_modules=None,\n",
        "#     llm_int8_enable_fp32_cpu_offload=False,\n",
        "# )\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     quantization_config=quantization_config,\n",
        "#     device_map=\"auto\",\n",
        "#     token=access_token,\n",
        "# )\n",
        "\n",
        "# system_prompt = \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "# user_input = \"Qual é a capital do Brasil?\"\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": system_prompt},\n",
        "#     {\"role\": \"user\", \"content\": user_input},\n",
        "# ]\n",
        "\n",
        "# chat_template = tokenizer.chat_template or \"\"\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "# ).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output_ids = model.generate(\n",
        "#         input_ids,\n",
        "#         max_length=512,\n",
        "#         temperature=0.7,\n",
        "#         top_p=0.9,\n",
        "#         do_sample=True,\n",
        "#         eos_token_id=tokenizer.eos_token_id,\n",
        "#         pad_token_id=tokenizer.pad_token_id,\n",
        "#     )\n",
        "\n",
        "# response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:],\n",
        "#                             skip_special_tokens=True)\n",
        "\n",
        "# print(\"Pergunta:\", user_input)\n",
        "# print(\"\\nResposta do modelo:\")\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\nTamanho do modelo em MB:\", model.get_memory_footprint() / (1024 * 1024))\n",
        "# print(\"Tipo de dispositivo:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict(user_input):\n",
        "#     system_prompt = \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": system_prompt},\n",
        "#         {\"role\": \"user\", \"content\": user_input},\n",
        "#     ]\n",
        "\n",
        "#     input_ids = tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=True,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         output_ids = model.generate(\n",
        "#             input_ids,\n",
        "#             max_length=512,\n",
        "#             temperature=0.7,\n",
        "#             top_p=0.9,\n",
        "#             do_sample=True,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.pad_token_id,\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n",
        "\n",
        "# result = predict(\"Qual é a capital do Brasil?\")\n",
        "# print(\"Resposta:\", result)"
      ],
      "metadata": {
        "id": "rwnqZ8p35sSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class LlamaQuantizedModel:\n",
        "    def __init__(self, model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", system_prompt=None):\n",
        "        self.model_id = model_id\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        self.logger.info(f\"Inicializando LlamaQuantizedModel com modelo: {model_id}\")\n",
        "\n",
        "        self.access_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "        if not self.access_token:\n",
        "            self.logger.error(\"HF_TOKEN não encontrado nas variáveis de ambiente\")\n",
        "            raise ValueError(\"HF_TOKEN não encontrado nas variáveis de ambiente\")\n",
        "\n",
        "        self.logger.info(\"Token do Hugging Face carregado com sucesso\")\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.logger.info(f\"Dispositivo utilizado: {self.device}\")\n",
        "\n",
        "        self.system_prompt = system_prompt or \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        print(f\"Carregando tokenizer para {self.model_id}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, token=self.access_token)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(f\"Carregando modelo quantizado para {self.model_id}...\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            token=self.access_token\n",
        "        )\n",
        "\n",
        "        print(\"Modelo carregado com sucesso!\")\n",
        "\n",
        "    def predict(self, user_input, max_length=512, temperature=0.7, top_p=0.9):\n",
        "        self.logger.info(f\"Iniciando predição com input: {user_input[:50]}...\")\n",
        "        self.logger.debug(f\"Parâmetros - max_length: {max_length}, temperature: {temperature}, top_p: {top_p}\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "\n",
        "        self.logger.debug(\"Aplicando chat template e tokenizando...\")\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.logger.info(f\"Shape dos tokens de entrada: {input_ids.shape}\")\n",
        "\n",
        "        self.logger.debug(\"Gerando output do modelo...\")\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        self.logger.debug(\"Decodificando resposta...\")\n",
        "        response = self.tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "        self.logger.info(f\"Predição completada. Resposta com {len(response)} caracteres\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    def set_system_prompt(self, new_prompt):\n",
        "        self.logger.info(f\"Alterando system prompt\")\n",
        "        self.system_prompt = new_prompt\n",
        "        self.logger.debug(f\"Novo system prompt: {new_prompt}\")\n",
        "\n",
        "    def get_device_info(self):\n",
        "        self.logger.info(\"Coletando informações do dispositivo\")\n",
        "        info = {\n",
        "            \"device\": str(self.device),\n",
        "            \"memory_allocated_gb\": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,\n",
        "        }\n",
        "        self.logger.debug(f\"Informações do dispositivo: {info}\")\n",
        "        return info\n",
        "\n",
        "    def predict_stream(self, user_input, max_length=512, temperature=0.7, top_p=0.9):\n",
        "        self.logger.info(f\"Iniciando predição com streaming para input: {user_input[:50]}...\")\n",
        "        self.logger.debug(f\"Parâmetros - max_length: {max_length}, temperature: {temperature}, top_p: {top_p}\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "\n",
        "        self.logger.debug(\"Aplicando chat template e tokenizando...\")\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.logger.info(f\"Shape dos tokens de entrada: {input_ids.shape}\")\n",
        "\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer,\n",
        "            skip_prompt=True,\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "\n",
        "        self.logger.debug(\"Iniciando geração com streaming em thread separada...\")\n",
        "        generation_kwargs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"streamer\": streamer,\n",
        "            \"max_length\": max_length,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"do_sample\": True,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "        }\n",
        "\n",
        "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        self.logger.info(\"Transmitindo tokens em tempo real...\")\n",
        "        return streamer\n",
        "\n",
        "    def unload_model(self):\n",
        "        self.logger.info(\"Iniciando descarregamento do modelo\")\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            del self.tokenizer\n",
        "            torch.cuda.empty_cache()\n",
        "            self.logger.info(\"Modelo descarregado com sucesso da memória\")\n"
      ],
      "metadata": {
        "id": "SyXnUn8u784N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_model = LlamaQuantizedModel()\n",
        "\n",
        "result = llama_model.predict(\"Qual é a capital do Brasil?\")\n",
        "print(\"Resposta:\", result)\n",
        "\n",
        "device_info = llama_model.get_device_info()\n",
        "print(\"\\nInformações do dispositivo:\", device_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157,
          "referenced_widgets": [
            "2e684e05e9884e05a684a7a6a2d479c8",
            "485314ff43c84f1795ec62f639e9f4b4",
            "efef74cc19e64d24b50b25980457401e",
            "c71e7b435a7a437d99e62b9c060bccb9",
            "32a3d65ade6541408f3c937301ec08a2",
            "157dad2c04884223b9ceeee553625de8",
            "0d8f4ec9e6e54e50af980ab4ab26af86",
            "abba06bccc2e490abe0cb68119a62f89",
            "f0e4fc03b5764235aea3383f4274e9f1",
            "8825e10ddc974599850f213aace73229",
            "0c8dbb4f6fb44483a8f3e51f48258a8f"
          ]
        },
        "id": "txkT_02W8jZE",
        "outputId": "0cd22f31-068a-4f0b-f391-c952381cf392"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando tokenizer para meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "Carregando modelo quantizado para meta-llama/Meta-Llama-3-8B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e684e05e9884e05a684a7a6a2d479c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso!\n",
            "Resposta: A capital do Brasil é Brasília!\n",
            "\n",
            "Informações do dispositivo: {'device': 'cuda', 'memory_allocated_gb': 10.636491775512695}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = llama_model.predict(\"Qual é a capital do Brasil?\")\n",
        "print(\"Resposta:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K8LamTo8ld5",
        "outputId": "9366bcbf-149e-4596-bde4-51a55f001246"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: A capital do Brasil é Brasília!\n",
            "CPU times: user 1.13 s, sys: 0 ns, total: 1.13 s\n",
            "Wall time: 3.06 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = llama_model.predict(\"o que você faz?\")\n",
        "print(\"Resposta:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZNnxh6z9uex",
        "outputId": "a6da18ec-cd65-4e66-c9fe-a292404aa8c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: Eu sou um assistente prestativo treinado para ajudar com informações e tarefas em português brasileiro. Eu posso:\n",
            "\n",
            "* Responder perguntas sobre um assunto específico ou geral\n",
            "* Fornecer informações sobre um tema ou tópico\n",
            "* Ajuda a encontrar recursos online ou offline\n",
            "* Traduzir textos simples de uma língua para outra (não é uma tradução profissional, mas pode ajudar em casos simples)\n",
            "* Geração de textos, como emails, cartas ou resumos\n",
            "* Simular uma conversa, como se fosse um bate-papo\n",
            "* Fornecer sugestões e ideias para resolver problemas ou melhorar situações\n",
            "\n",
            "Se você tiver alguma dúvida ou necessidade específica, sinta-se à vontade para perguntar!\n",
            "CPU times: user 20.8 s, sys: 0 ns, total: 20.8 s\n",
            "Wall time: 37.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "streamer = llama_model.predict_stream(\"Me conte uma história interessante em 100 palavras\")\n",
        "\n",
        "print(\"Resposta com streaming: \", end=\"\", flush=True)\n",
        "for text in streamer:\n",
        "    print(text, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeJdZksd_8hW",
        "outputId": "90b4379c-6b75-4a7b-da61-6d5a667e07a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta com streaming: Que delícia! Aqui vai uma história interessante para você:\n",
            "\n",
            "Em 1943, um homem chamado Victor Frankl foi enviado para um campo de concentração nazista. Ele era um psiquiatra e um escritor, e sua experiência lá inspirou seu livro mais famoso, \"Man's Search for Meaning\". Durante sua internação, Victor descobriu que, apesar da perda de tudo que ele conhecera, havia algo que o mantinha vivo: a busca por significado. Ele começou a dar aulas para os outros prisioneiros sobre a importância da busca por significado, e isso os ajudou a sobreviver àqueles tempos difíceis.\n",
            "\n",
            "CPU times: user 13.3 s, sys: 76.5 ms, total: 13.4 s\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adWYecB-CcjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}