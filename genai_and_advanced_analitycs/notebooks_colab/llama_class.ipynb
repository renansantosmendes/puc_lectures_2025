{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e684e05e9884e05a684a7a6a2d479c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485314ff43c84f1795ec62f639e9f4b4",
              "IPY_MODEL_efef74cc19e64d24b50b25980457401e",
              "IPY_MODEL_c71e7b435a7a437d99e62b9c060bccb9"
            ],
            "layout": "IPY_MODEL_32a3d65ade6541408f3c937301ec08a2"
          }
        },
        "485314ff43c84f1795ec62f639e9f4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_157dad2c04884223b9ceeee553625de8",
            "placeholder": "​",
            "style": "IPY_MODEL_0d8f4ec9e6e54e50af980ab4ab26af86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "efef74cc19e64d24b50b25980457401e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abba06bccc2e490abe0cb68119a62f89",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e4fc03b5764235aea3383f4274e9f1",
            "value": 4
          }
        },
        "c71e7b435a7a437d99e62b9c060bccb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8825e10ddc974599850f213aace73229",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8dbb4f6fb44483a8f3e51f48258a8f",
            "value": " 4/4 [01:08&lt;00:00, 15.17s/it]"
          }
        },
        "32a3d65ade6541408f3c937301ec08a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157dad2c04884223b9ceeee553625de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8f4ec9e6e54e50af980ab4ab26af86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abba06bccc2e490abe0cb68119a62f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e4fc03b5764235aea3383f4274e9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8825e10ddc974599850f213aace73229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8dbb4f6fb44483a8f3e51f48258a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2149cd937a1a45cda9b163f5cbd20b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae5edc7c27e448729c9670661a88d579",
              "IPY_MODEL_5c83753459324b1d8f3b8b60b399fbca",
              "IPY_MODEL_d9af5c7917fa485d9e6e63b5f12ae7e3"
            ],
            "layout": "IPY_MODEL_802b9480973d432f9da32610008d342b"
          }
        },
        "ae5edc7c27e448729c9670661a88d579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93c86f51eb24fa99f79da5dfd5018da",
            "placeholder": "​",
            "style": "IPY_MODEL_5b21ece28b184a6dab008b029c067552",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5c83753459324b1d8f3b8b60b399fbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d307709a76746b5a4ddd3d8e7d94a78",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96dcd77958ce40e5a7eba976b041931c",
            "value": 4
          }
        },
        "d9af5c7917fa485d9e6e63b5f12ae7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72e370f1f99b46d4987ec24e57c7dec9",
            "placeholder": "​",
            "style": "IPY_MODEL_3b4017a854f9455db3f327b97f621038",
            "value": " 4/4 [01:26&lt;00:00, 18.75s/it]"
          }
        },
        "802b9480973d432f9da32610008d342b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93c86f51eb24fa99f79da5dfd5018da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b21ece28b184a6dab008b029c067552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d307709a76746b5a4ddd3d8e7d94a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96dcd77958ce40e5a7eba976b041931c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72e370f1f99b46d4987ec24e57c7dec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4017a854f9455db3f327b97f621038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IIz16-D5eks",
        "outputId": "094e9eae-69a7-4c21-d7ba-5c1b25cd25a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g929fUPV5X6I"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "# from torch.quantization import quantize_dynamic\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# access_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_8bit=True,\n",
        "#     llm_int8_threshold=6.0,\n",
        "#     llm_int8_skip_modules=None,\n",
        "#     llm_int8_enable_fp32_cpu_offload=False,\n",
        "# )\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     quantization_config=quantization_config,\n",
        "#     device_map=\"auto\",\n",
        "#     token=access_token,\n",
        "# )\n",
        "\n",
        "# system_prompt = \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "# user_input = \"Qual é a capital do Brasil?\"\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": system_prompt},\n",
        "#     {\"role\": \"user\", \"content\": user_input},\n",
        "# ]\n",
        "\n",
        "# chat_template = tokenizer.chat_template or \"\"\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "# ).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output_ids = model.generate(\n",
        "#         input_ids,\n",
        "#         max_length=512,\n",
        "#         temperature=0.7,\n",
        "#         top_p=0.9,\n",
        "#         do_sample=True,\n",
        "#         eos_token_id=tokenizer.eos_token_id,\n",
        "#         pad_token_id=tokenizer.pad_token_id,\n",
        "#     )\n",
        "\n",
        "# response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:],\n",
        "#                             skip_special_tokens=True)\n",
        "\n",
        "# print(\"Pergunta:\", user_input)\n",
        "# print(\"\\nResposta do modelo:\")\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\nTamanho do modelo em MB:\", model.get_memory_footprint() / (1024 * 1024))\n",
        "# print(\"Tipo de dispositivo:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict(user_input):\n",
        "#     system_prompt = \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": system_prompt},\n",
        "#         {\"role\": \"user\", \"content\": user_input},\n",
        "#     ]\n",
        "\n",
        "#     input_ids = tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=True,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         output_ids = model.generate(\n",
        "#             input_ids,\n",
        "#             max_length=512,\n",
        "#             temperature=0.7,\n",
        "#             top_p=0.9,\n",
        "#             do_sample=True,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.pad_token_id,\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n",
        "\n",
        "# result = predict(\"Qual é a capital do Brasil?\")\n",
        "# print(\"Resposta:\", result)"
      ],
      "metadata": {
        "id": "rwnqZ8p35sSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class LlamaQuantizedModel:\n",
        "    def __init__(self, model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", system_prompt=None):\n",
        "        self.model_id = model_id\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        self.logger.info(f\"Inicializando LlamaQuantizedModel com modelo: {model_id}\")\n",
        "\n",
        "        self.access_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "        if not self.access_token:\n",
        "            self.logger.error(\"HF_TOKEN não encontrado nas variáveis de ambiente\")\n",
        "            raise ValueError(\"HF_TOKEN não encontrado nas variáveis de ambiente\")\n",
        "\n",
        "        self.logger.info(\"Token do Hugging Face carregado com sucesso\")\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.logger.info(f\"Dispositivo utilizado: {self.device}\")\n",
        "\n",
        "        self.system_prompt = system_prompt or \"Você é um assistente prestativo que responde em português brasileiro.\"\n",
        "\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        print(f\"Carregando tokenizer para {self.model_id}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, token=self.access_token)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(f\"Carregando modelo quantizado para {self.model_id}...\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            token=self.access_token\n",
        "        )\n",
        "\n",
        "        print(\"Modelo carregado com sucesso!\")\n",
        "\n",
        "    def predict(self, user_input, max_length=512, temperature=0.7, top_p=0.9):\n",
        "        self.logger.info(f\"Iniciando predição com input: {user_input[:50]}...\")\n",
        "        self.logger.debug(f\"Parâmetros - max_length: {max_length}, temperature: {temperature}, top_p: {top_p}\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "\n",
        "        self.logger.debug(\"Aplicando chat template e tokenizando...\")\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.logger.info(f\"Shape dos tokens de entrada: {input_ids.shape}\")\n",
        "\n",
        "        self.logger.debug(\"Gerando output do modelo...\")\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        self.logger.debug(\"Decodificando resposta...\")\n",
        "        response = self.tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "        self.logger.info(f\"Predição completada. Resposta com {len(response)} caracteres\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    def set_system_prompt(self, new_prompt):\n",
        "        self.logger.info(f\"Alterando system prompt\")\n",
        "        self.system_prompt = new_prompt\n",
        "        self.logger.debug(f\"Novo system prompt: {new_prompt}\")\n",
        "\n",
        "    def get_device_info(self):\n",
        "        self.logger.info(\"Coletando informações do dispositivo\")\n",
        "        info = {\n",
        "            \"device\": str(self.device),\n",
        "            \"memory_allocated_gb\": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,\n",
        "        }\n",
        "        self.logger.debug(f\"Informações do dispositivo: {info}\")\n",
        "        return info\n",
        "\n",
        "    def predict_stream(self, user_input, max_length=512, temperature=0.7, top_p=0.9):\n",
        "        self.logger.info(f\"Iniciando predição com streaming para input: {user_input[:50]}...\")\n",
        "        self.logger.debug(f\"Parâmetros - max_length: {max_length}, temperature: {temperature}, top_p: {top_p}\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "\n",
        "        self.logger.debug(\"Aplicando chat template e tokenizando...\")\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.logger.info(f\"Shape dos tokens de entrada: {input_ids.shape}\")\n",
        "\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer,\n",
        "            skip_prompt=True,\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "\n",
        "        self.logger.debug(\"Iniciando geração com streaming em thread separada...\")\n",
        "        generation_kwargs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"streamer\": streamer,\n",
        "            \"max_length\": max_length,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"do_sample\": True,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "        }\n",
        "\n",
        "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        self.logger.info(\"Transmitindo tokens em tempo real...\")\n",
        "        return streamer\n",
        "\n",
        "    def unload_model(self):\n",
        "        self.logger.info(\"Iniciando descarregamento do modelo\")\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            del self.tokenizer\n",
        "            torch.cuda.empty_cache()\n",
        "            self.logger.info(\"Modelo descarregado com sucesso da memória\")\n"
      ],
      "metadata": {
        "id": "SyXnUn8u784N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_model = LlamaQuantizedModel()\n",
        "\n",
        "result = llama_model.predict(\"Qual é a capital do Brasil?\")\n",
        "print(\"Resposta:\", result)\n",
        "\n",
        "device_info = llama_model.get_device_info()\n",
        "print(\"\\nInformações do dispositivo:\", device_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157,
          "referenced_widgets": [
            "2e684e05e9884e05a684a7a6a2d479c8",
            "485314ff43c84f1795ec62f639e9f4b4",
            "efef74cc19e64d24b50b25980457401e",
            "c71e7b435a7a437d99e62b9c060bccb9",
            "32a3d65ade6541408f3c937301ec08a2",
            "157dad2c04884223b9ceeee553625de8",
            "0d8f4ec9e6e54e50af980ab4ab26af86",
            "abba06bccc2e490abe0cb68119a62f89",
            "f0e4fc03b5764235aea3383f4274e9f1",
            "8825e10ddc974599850f213aace73229",
            "0c8dbb4f6fb44483a8f3e51f48258a8f"
          ]
        },
        "id": "txkT_02W8jZE",
        "outputId": "0cd22f31-068a-4f0b-f391-c952381cf392"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando tokenizer para meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "Carregando modelo quantizado para meta-llama/Meta-Llama-3-8B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e684e05e9884e05a684a7a6a2d479c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso!\n",
            "Resposta: A capital do Brasil é Brasília!\n",
            "\n",
            "Informações do dispositivo: {'device': 'cuda', 'memory_allocated_gb': 10.636491775512695}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = llama_model.predict(\"Qual é a capital do Brasil?\")\n",
        "print(\"Resposta:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K8LamTo8ld5",
        "outputId": "9366bcbf-149e-4596-bde4-51a55f001246"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: A capital do Brasil é Brasília!\n",
            "CPU times: user 1.13 s, sys: 0 ns, total: 1.13 s\n",
            "Wall time: 3.06 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = llama_model.predict(\"o que você faz?\")\n",
        "print(\"Resposta:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZNnxh6z9uex",
        "outputId": "a6da18ec-cd65-4e66-c9fe-a292404aa8c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: Eu sou um assistente prestativo treinado para ajudar com informações e tarefas em português brasileiro. Eu posso:\n",
            "\n",
            "* Responder perguntas sobre um assunto específico ou geral\n",
            "* Fornecer informações sobre um tema ou tópico\n",
            "* Ajuda a encontrar recursos online ou offline\n",
            "* Traduzir textos simples de uma língua para outra (não é uma tradução profissional, mas pode ajudar em casos simples)\n",
            "* Geração de textos, como emails, cartas ou resumos\n",
            "* Simular uma conversa, como se fosse um bate-papo\n",
            "* Fornecer sugestões e ideias para resolver problemas ou melhorar situações\n",
            "\n",
            "Se você tiver alguma dúvida ou necessidade específica, sinta-se à vontade para perguntar!\n",
            "CPU times: user 20.8 s, sys: 0 ns, total: 20.8 s\n",
            "Wall time: 37.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "streamer = llama_model.predict_stream(\"Me conte uma história interessante em 100 palavras\")\n",
        "\n",
        "print(\"Resposta com streaming: \", end=\"\", flush=True)\n",
        "for text in streamer:\n",
        "    print(text, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeJdZksd_8hW",
        "outputId": "90b4379c-6b75-4a7b-da61-6d5a667e07a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta com streaming: Que delícia! Aqui vai uma história interessante para você:\n",
            "\n",
            "Em 1943, um homem chamado Victor Frankl foi enviado para um campo de concentração nazista. Ele era um psiquiatra e um escritor, e sua experiência lá inspirou seu livro mais famoso, \"Man's Search for Meaning\". Durante sua internação, Victor descobriu que, apesar da perda de tudo que ele conhecera, havia algo que o mantinha vivo: a busca por significado. Ele começou a dar aulas para os outros prisioneiros sobre a importância da busca por significado, e isso os ajudou a sobreviver àqueles tempos difíceis.\n",
            "\n",
            "CPU times: user 13.3 s, sys: 76.5 ms, total: 13.4 s\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXEMPLO 2 - usando langchain"
      ],
      "metadata": {
        "id": "MxBbg9SSDXc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes langchain langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdqPu0DvDfnl",
        "outputId": "78bb3741-0c1c-4db1-c379-c5eaeb769d0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "c17tjWmeE6HQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.outputs.llm_result import LLMResult\n",
        "from langchain_core.outputs.generation import GenerationChunk\n",
        "from pydantic import Field, PrivateAttr # Import Field and PrivateAttr\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class LlamaQuantizedModel(LLM):\n",
        "    # Define model_id and system_prompt as Pydantic fields\n",
        "    model_id: str = Field(default=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "    system_prompt: str = Field(default=\"Você é um assistente prestativo que responde em português brasileiro.\")\n",
        "\n",
        "    # Use PrivateAttr for internal attributes that should not be part of the Pydantic model's state\n",
        "    _logger: logging.Logger = PrivateAttr(default_factory=lambda: logging.getLogger(__name__))\n",
        "    _access_token: str = PrivateAttr(default=None)\n",
        "    _device: torch.device = PrivateAttr(default=None)\n",
        "    _tokenizer: AutoTokenizer = PrivateAttr(default=None)\n",
        "    _model: AutoModelForCausalLM = PrivateAttr(default=None)\n",
        "\n",
        "    def __init__(self, **data):\n",
        "        super().__init__(**data) # This initializes model_id and system_prompt\n",
        "        self._initialize_internal_state()\n",
        "\n",
        "    def _initialize_internal_state(self):\n",
        "        self._logger.info(f\"Inicializando LlamaQuantizedModel com modelo: {self.model_id}\")\n",
        "\n",
        "        # Use _access_token for internal state, getting it from HF_TOKEN as set in previous cells\n",
        "        self._access_token = os.getenv(\"HF_TOKEN\") # Corrected: use HF_TOKEN\n",
        "\n",
        "        if not self._access_token:\n",
        "            self._logger.error(\"HF_TOKEN não encontrado nas variáveis de ambiente\") # Corrected\n",
        "            raise ValueError(\"HF_TOKEN não encontrado nas variáveis de ambiente\") # Corrected\n",
        "\n",
        "        self._logger.info(\"Token do Hugging Face carregado com sucesso\")\n",
        "\n",
        "        # Use _device for internal state\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self._logger.info(f\"Dispositivo utilizado: {self._device}\")\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    # Property getters for consistency if other methods will refer to them as properties\n",
        "    @property\n",
        "    def logger(self):\n",
        "        return self._logger\n",
        "\n",
        "    @property\n",
        "    def access_token(self):\n",
        "        return self._access_token\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self._device\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"llama_quantized\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        self.logger.info(f\"Carregando tokenizer para {self.model_id}...\")\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(self.model_id, token=self.access_token)\n",
        "        self._tokenizer.pad_token = self._tokenizer.eos_token\n",
        "\n",
        "        self.logger.info(f\"Carregando modelo quantizado para {self.model_id}...\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        self._model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            token=self.access_token,\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"Modelo carregado com sucesso!\")\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop=None,\n",
        "        run_manager=None,\n",
        "        **kwargs,\n",
        "    ) -> str:\n",
        "        self.logger.info(f\"Chamada _call com prompt: {prompt[:50]}...\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=kwargs.get(\"max_length\", 512),\n",
        "                temperature=kwargs.get(\"temperature\", 0.7),\n",
        "                top_p=kwargs.get(\"top_p\", 0.9),\n",
        "                do_sample=True,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "        self.logger.info(f\"Resposta gerada com {len(response)} caracteres\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop=None,\n",
        "        run_manager=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.logger.info(f\"Chamada _stream com prompt: {prompt[:50]}...\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer,\n",
        "            skip_prompt=True,\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "\n",
        "        generation_kwargs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"streamer\": streamer,\n",
        "            \"max_length\": kwargs.get(\"max_length\", 512),\n",
        "            \"temperature\": kwargs.get(\"temperature\", 0.7),\n",
        "            \"top_p\": kwargs.get(\"top_p\", 0.9),\n",
        "            \"do_sample\": True,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "        }\n",
        "\n",
        "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        self.logger.info(\"Transmitindo tokens em tempo real...\")\n",
        "\n",
        "        for text in streamer:\n",
        "             yield GenerationChunk(text=text)\n",
        "\n",
        "    def set_system_prompt(self, new_prompt):\n",
        "        self.logger.info(f\"Alterando system prompt\")\n",
        "        self.system_prompt = new_prompt\n",
        "        self.logger.debug(f\"Novo system prompt: {new_prompt}\")\n",
        "\n",
        "    def get_device_info(self):\n",
        "        self.logger.info(\"Coletando informações do dispositivo\")\n",
        "        info = {\n",
        "            \"device\": str(self.device),\n",
        "            \"memory_allocated_gb\": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,\n",
        "        }\n",
        "        self.logger.debug(f\"Informações do dispositivo: {info}\")\n",
        "        return info\n",
        "\n",
        "    def unload_model(self):\n",
        "        self.logger.info(\"Iniciando descarregamento do modelo\")\n",
        "        if self._model is not None:\n",
        "            del self._model\n",
        "            del self._tokenizer\n",
        "            torch.cuda.empty_cache()\n",
        "            self.logger.info(\"Modelo descarregado com sucesso da memória\")"
      ],
      "metadata": {
        "id": "adWYecB-CcjD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llama_model = LlamaQuantizedModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "2149cd937a1a45cda9b163f5cbd20b1a",
            "ae5edc7c27e448729c9670661a88d579",
            "5c83753459324b1d8f3b8b60b399fbca",
            "d9af5c7917fa485d9e6e63b5f12ae7e3",
            "802b9480973d432f9da32610008d342b",
            "a93c86f51eb24fa99f79da5dfd5018da",
            "5b21ece28b184a6dab008b029c067552",
            "7d307709a76746b5a4ddd3d8e7d94a78",
            "96dcd77958ce40e5a7eba976b041931c",
            "72e370f1f99b46d4987ec24e57c7dec9",
            "3b4017a854f9455db3f327b97f621038"
          ]
        },
        "id": "ya0iTuJ2DnWz",
        "outputId": "5744cd75-5453-4a08-96cc-f6f9906e9daf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2149cd937a1a45cda9b163f5cbd20b1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.2 s, sys: 10.8 s, total: 23 s\n",
            "Wall time: 1min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = llama_model.invoke(\"Qual é a capital do Brasil?\")\n",
        "print(resultado)\n",
        "\n",
        "for chunk in llama_model.stream(\"Me conte uma história\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwozlmvGDreU",
        "outputId": "4d79cf43-69d9-4aec-d2ac-e599f25b121a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A capital do Brasil é Brasília!\n",
            "Que prazer! Aqui vai uma história para você:\n",
            "\n",
            "Havia uma vez, em um pequeno vilarejo no interior do Brasil, uma jovem chamada Maria. Maria era uma pessoa muito especial, pois possuía um dom incrível: ela podia falar com os animais.\n",
            "\n",
            "Uma tarde, enquanto trabalhava na horta de sua família, Maria ouviu um choramingo vindo da floresta adjacente. Ela se aproximou do som e descobriu um pequeno urso, ferido e sozinho. Maria, com um coração compassivo, decidiu ajudar o urso a se recuperar. Ela o levou para sua casa e cuidou dele como se fosse um filho.\n",
            "\n",
            "O urso, que Maria batizou de Ursinho, rapidamente se recuperou e começou a ajudar a família de Maria com tarefas como coletar frutas e caçar pequenos animais. Em troca, Maria aprendeu a linguagem dos animais e começou a entender melhor o mundo natural.\n",
            "\n",
            "Com o tempo, Ursinho se tornou um amigo inseparável de Maria. Juntos, eles exploravam a floresta, descobrindo segredos e maravilhas do mundo natural. E, apesar de ser um urso, Ursinho era um amigo muito leal e fiel a Maria.\n",
            "\n",
            "Essa história é uma lembrança de como a compaixão e a amizade podem conectar seres diferentes, mesmo quando parecem impossíveis."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llama_model.invoke(\"que é ia generativa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "bmmQVxbOGeeU",
        "outputId": "b959ca59-a55e-4f31-90b2-18c31556a680"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 52.2 s, sys: 2.79 s, total: 55 s\n",
            "Wall time: 52.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Excelente pergunta!\\n\\nA inteligência artificial (IA) gerativa, também conhecida como geração de texto por IA, é um tipo de tecnologia que permite criar texto original e coerente com base em inputs ou prompts. Isso significa que, ao invés de apenas processar e analisar texto, a IA gerativa pode criar texto novo e significativo.\\n\\nEssa tecnologia é baseada em algoritmos de aprendizado de máquina e em grandes volumes de dados treinados. Ao processar esses dados, a IA gerativa pode aprender a reconhecer padrões, estilos e estruturas de linguagem, o que permite que ela crie texto que seja semelhante ao que foi treinado.\\n\\nA IA gerativa tem inúmeras aplicações, como:\\n\\n* Geração de conteúdo automático para websites, blogs e mídias sociais;\\n* Criar resumos e sinopses de textos longos;\\n* Gerar ideias e sugestões para escritores e criativos;\\n* Simular conversas e respostas a perguntas;\\n* Criar relatórios e documentos com base em dados.\\n\\nNo entanto, é importante notar que a IA gerativa ainda é um campo em desenvolvimento e que a qualidade do texto gerado pode variar dependendo do nível de treinamento e do tipo de modelo utilizado. Além disso, é fundamental garantir a autenticidade e a precisão do conteúdo gerado, evitando a possibilidade de conteúdo falso ou enganoso.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtajND1lGktV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}